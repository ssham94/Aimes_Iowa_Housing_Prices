{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement:  \n",
    "Can we find which variables are the most important to look for while buying/investing in a house in Ames Iowa? Can we create a predictive model to predict house sale prices dependent on certain variables\n",
    "\n",
    "### Models used/tested:\n",
    "Linear Regression  \n",
    "Ridge Regression (Final model chosen at end based on RMSE and R Squared scores)  \n",
    "Lasso Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "df_train = pd.read_csv('./datasets/train.csv') # building model around\n",
    "df_test = pd.read_csv('./datasets/test.csv') # set to use to generate predictions using model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking size of train data\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking size of test data\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the missing column (SalePrice)\n",
    "set(df_train.columns) - set(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the first few lines of data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for all the different variables allowed to use\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set row names to be equal to the Id numbers\n",
    "df_train.set_index('Id', inplace = True)\n",
    "df_test.set_index('Id', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and dealing with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "df_train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the columns which will be dropped based on number of null values (ie, pool QC, since over 99% is missing \n",
    "# we can drop it)\n",
    "dropColumns = ['Pool QC', 'Misc Feature', 'Alley', 'Pool Area']\n",
    "df_train = df_train.drop(columns = df_train[dropColumns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping same columns in test data\n",
    "df_test = df_test.drop(columns = df_test[dropColumns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if they dropped correctly\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of ordinal variables\n",
    "# List was found using data dictionary on kaggle\n",
    "ordinal_cols = ['Lot Shape','Utilities','Land Slope','Exter Qual','Exter Cond',\n",
    "               'Bsmt Cond','Bsmt Qual','Bsmt Exposure','BsmtFin Type 1','BsmtFin Type 2',\n",
    "               'Heating QC','Electrical','Kitchen Qual','Functional','Fireplace Qu',\n",
    "                'Garage Finish','Garage Qual','Garage Cond','Paved Drive','Fence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary to convert ordinal variables\n",
    "# This section of code was adapted by my classmate Noah Christiansen\n",
    "rank_dict = {'Lot Shape': {'IR1': 1, 'Reg': 4, 'IR2': 2, 'IR3': 3},\n",
    "             'Utilities': {'AllPub':4, 'NoSeWa':2, 'NoSewr':3, 'ELO':1},\n",
    "             'Land Slope': {'Gtl':3, 'Sev':2, 'Mod':1},\n",
    "             'Exter Qual': {'Gd':4, 'TA':3, 'Ex':5, 'Fa':2,'Po':1},\n",
    "             'Exter Cond': {'TA':3, 'Gd':4, 'Fa':2, 'Ex':5, 'Po':1},\n",
    "             'Bsmt Cond': {'TA':3, 'Gd':4, np.nan:0, 'Fa':2, 'Po':1, 'Ex':5},\n",
    "             'Bsmt Qual': {'TA':3, 'Gd':4, 'Fa':2, np.nan:0, 'Ex':5, 'Po':1},\n",
    "             'Bsmt Exposure': {'No':1, 'Gd':4, 'Av':3, np.nan:0, 'Mn':2},\n",
    "             'BsmtFin Type 1': {'GLQ':6, 'Unf':1, 'ALQ':5, 'Rec':3, np.nan:0, 'BLQ':4, 'LwQ':2},\n",
    "             'BsmtFin Type 2': {'Unf':1, 'Rec':3, np.nan:0, 'BLQ':4, 'GLQ':6, 'LwQ':2, 'ALQ':5},\n",
    "             'Heating QC': {'Ex':5, 'TA':3, 'Gd':4, 'Fa':2, 'Po':1},\n",
    "             'Electrical': {'SBrkr':5, 'FuseF':3, 'FuseA':4, 'FuseP':2, 'Mix':1,np.nan:0},\n",
    "             'Kitchen Qual': {'Gd':4, 'TA':3, 'Fa':2, 'Ex':5,'Po':1},\n",
    "             'Functional': {'Typ':8, 'Mod':5, 'Min2':6, 'Maj1':4, 'Min1':7, 'Sev':2, 'Sal':1, 'Maj2':3},\n",
    "             'Fireplace Qu': {np.nan:0, 'TA':3, 'Gd':4, 'Po':1, 'Ex':5, 'Fa':2},\n",
    "             'Garage Finish': {'RFn':2, 'Unf':1, 'Fin':3, np.nan:0},\n",
    "             'Garage Qual': {'TA':3, 'Fa':2, np.nan:0, 'Gd':4, 'Ex':5, 'Po':1},\n",
    "             'Garage Cond': {'TA':3, 'Fa':2, np.nan:0, 'Po':1, 'Gd':4, 'Ex':5},\n",
    "             'Paved Drive': {'Y':3, 'N':1, 'P':2},\n",
    "             'Fence': {np.nan:0, 'MnPrv':3, 'GdPrv':4, 'GdWo':2, 'MnWw':1}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created function to convert ordinals to numbers instead of strings\n",
    "# This section of code was shared by my classmate Noah Christiansen\n",
    "def rank_ordinals(df, ordinalList):\n",
    "    for col in ordinalList:\n",
    "        df[col] = df[col].map(rank_dict[col])\n",
    "    return df\n",
    "\n",
    "rank_ordinals(df_train, ordinal_cols)\n",
    "rank_ordinals(df_test, ordinal_cols).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating columns to categorical columns\n",
    "catColumns = df_train.select_dtypes(include=['object']).columns\n",
    "catColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many nominal columns there are\n",
    "len(catColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating columns into numerical columns\n",
    "numColumns = df_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numColumnsTest = numColumns.drop('SalePrice')\n",
    "numColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all null values in numerical columns to 0\n",
    "df_train[numColumns] = df_train[numColumns].fillna(0)\n",
    "df_test[numColumnsTest] = df_test[numColumnsTest].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if all null values have been removed properly\n",
    "df_train[numColumns].isnull().sum().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if all null values have been removed properly in test data\n",
    "df_test[numColumnsTest].isnull().sum().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with null values in categorical columns\n",
    "df_train[catColumns] = df_train[catColumns].fillna('NA')\n",
    "df_test[catColumns] = df_test[catColumns].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if all null values have been removed properly\n",
    "df_train.isnull().sum().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if all null values have been removed properly in test data\n",
    "df_test.isnull().sum().sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see correlation heatmap to see what might be a large driver for saleprice\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.title('Linear Correlation to Price', fontsize = 15)\n",
    "\n",
    "\n",
    "sns.heatmap(df_train.corr()[['SalePrice']].sort_values(by = 'SalePrice', ascending = False), \n",
    "            annot = True, cmap = 'coolwarm',annot_kws={\"size\": 12})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After checking the correlation for the variables, we will only focus on the variables which have higher than 0.5 \n",
    "# linear correlation\n",
    "# All ordinal variables have been changed to a ranking system to give accurate representation of the variable\n",
    "# Dummy variables were created for all categorical variables\n",
    "# All these data variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From initial heatmap, couple of variables will have to be explored further:\n",
    "\n",
    "- Overall Qual \n",
    "- Gr Liv Area \n",
    "- Garage Area \n",
    "- Garage Cars \n",
    "- Total Bsmt SF \n",
    "- 1st Fl SF, 2nd Fl SF \n",
    "- Year Built \n",
    "- Year Remod/Built \n",
    "- Full Bath \n",
    "- TotRms Abv Gr \n",
    "- Fireplaces \n",
    "- BsmtFin SF1\n",
    "\n",
    "Other categorical data will have to be explored with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring data with higher correlations (Lot Area):\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Lot Area ft2')\n",
    "plt.ylabel('Prices sold')\n",
    "plt.title('Price sold by lot area')\n",
    "plt.scatter(df_train['Lot Area'], df_train['SalePrice'])\n",
    "\n",
    "# Semi linear relationship can be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers for lot area\n",
    "mask = df_train['Lot Area'] > 30000\n",
    "print(df_train[mask].shape)\n",
    "\n",
    "# Since it only has 23 columns, this is less than 1% of the total data\n",
    "df_train = df_train.drop(index = df_train[mask].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring data with higher correlations (Lot Area):\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Lot Area ft2')\n",
    "plt.ylabel('Prices sold')\n",
    "plt.title('Price sold by lot area')\n",
    "plt.scatter(df_train['Lot Area'], df_train['SalePrice'])\n",
    "b, m = polyfit(df_train['Lot Area'], df_train['SalePrice'], 1)\n",
    "plt.plot(df_train['Lot Area'], b + m * df_train['Lot Area'], '-', color = 'red')\n",
    "# Linear relationship can be seen once outliers have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column with total sq footage \n",
    "totalSQ = pd.DataFrame(df_train[['Total Bsmt SF', '1st Flr SF', '2nd Flr SF', 'Garage Area']])\n",
    "totalSQ['Total'] = totalSQ['Total Bsmt SF'] + totalSQ['1st Flr SF'] + totalSQ['2nd Flr SF'] + totalSQ['Garage Area'] \n",
    "totalSQ['SalePrice'] = df_train['SalePrice']\n",
    "totalSQ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating process for test data\n",
    "totalSQTest = pd.DataFrame(df_test[['Total Bsmt SF', '1st Flr SF', '2nd Flr SF', 'Garage Area']])\n",
    "totalSQTest['Total'] = totalSQTest['Total Bsmt SF'] + totalSQTest['1st Flr SF'] + totalSQTest['2nd Flr SF'] + totalSQTest['Garage Area'] \n",
    "totalSQTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing correlation between total square footage as well as individual breakdown by price\n",
    "sns.heatmap(totalSQ.corr()[['SalePrice', 'Total', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', '2nd Flr SF']].sort_values(by = 'SalePrice', ascending = False), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scatterplot for total square footage vs price\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Total Square Footage', fontsize = 20)\n",
    "plt.ylabel('Prices sold', fontsize = 20)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.title('Price by Total Square Footage', fontsize = 20)\n",
    "plt.scatter(totalSQ['Total'], totalSQ['SalePrice'])\n",
    "b, m = polyfit(totalSQ['Total'], totalSQ['SalePrice'], 1)\n",
    "plt.plot(totalSQ['Total'], b + m * totalSQ['Total'], '-', color = 'red')\n",
    "\n",
    "# Has generally good linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scatterplot for total square footage vs price\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Total Square Footage', fontsize = 20)\n",
    "plt.ylabel('Number Sold', fontsize = 20)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.title('Number of Houses Sold by Total Square Footage', fontsize = 20)\n",
    "arr = plt.hist(x=totalSQ['Total'], edgecolor='black', linewidth=1, bins = 8)\n",
    "for i in range(8):\n",
    "    plt.text(arr[1][i],arr[0][i],str(arr[0][i]), horizontalalignment='left', \n",
    "             verticalalignment = 'bottom', fontsize = 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the average house size as well as average price\n",
    "print('Average house size:', totalSQ['Total'].mean())\n",
    "print('Average price:', totalSQ['SalePrice'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it has a generally good trend as well as linear relationship, will add total sq to original dataframs\n",
    "df_train['Total SF'] = totalSQ['Total']\n",
    "df_test['Total SF'] = totalSQTest['Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scatter plot for liveable areas\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Liveable Area Above Ground')\n",
    "plt.ylabel('Prices sold')\n",
    "plt.title('Price by Liveable Area Above Ground')\n",
    "plt.scatter(df_train['Gr Liv Area'], df_train['SalePrice'])\n",
    "b, m = polyfit(df_train['Gr Liv Area'], df_train['SalePrice'], 1)\n",
    "plt.plot(df_train['Gr Liv Area'], b + m * df_train['Gr Liv Area'], '-', color = 'red')\n",
    "# Has generally good linear function, after other outliers have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring data with higher correlations (overall quality):\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.xlabel('Overall quality rating')\n",
    "plt.ylabel('Prices sold')\n",
    "plt.title('Price by quality rating')\n",
    "plt.scatter(df_train['Overall Qual'], df_train['SalePrice'])\n",
    "\n",
    "# Can see linear correlation between quality of house and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for the neighborhood variable to check \n",
    "overQual = pd.get_dummies(df_train['Overall Qual'])\n",
    "overQual['SalePrice'] = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average price by neighborhood\n",
    "overMeans = []\n",
    "for column in overQual.columns:\n",
    "    mask = df_train['Overall Qual'] == column\n",
    "    temp = df_train[mask]['SalePrice'].mean()\n",
    "    overMeans.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe for easier manipulation\n",
    "overallQualMean = pd.DataFrame()\n",
    "overallQualMean['Overall Qual'] = overQual.columns.values\n",
    "overallQualMean['Average Price'] = overMeans\n",
    "overallQualMean.drop(index = 10, inplace = True)\n",
    "overallQualMean = overallQualMean.sort_values('Average Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataframe and seeing average prices\n",
    "overallQualMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning lists to be used in histogram plot, and plotting as well\n",
    "overallList = list(overallQualMean['Overall Qual'].values)\n",
    "avgPriceList = list(overallQualMean['Average Price'].values)\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.xticks(range(1,11), fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlabel('Quality Rating', fontsize = 20)\n",
    "plt.ylabel('Price', fontsize = 20)\n",
    "plt.title('Average Price by Overall Quality', fontsize = 20)\n",
    "plt.bar(overallList, avgPriceList,align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Year Remod/Add data, since this is a bigger indicator than year built, it is a bigger driver for price\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.xlabel('Year Remodeled/Built', fontsize = 20)\n",
    "plt.ylabel('Price', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Price by Year Remodeled', fontsize = 20)\n",
    "plt.scatter(df_train['Year Remod/Add'], df_train['SalePrice'])\n",
    "b, m = polyfit(df_train['Year Remod/Add'], df_train['SalePrice'], 1)\n",
    "plt.plot(df_train['Year Remod/Add'], b + m * df_train['Year Remod/Add'], '-', color = 'red')\n",
    "# Slight linear relatipnship can be see. More recent renovation = generally higher price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Year Remod/Add data, since this is a bigger indicator than year built, it is a bigger driver for price\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.xlabel('Year Built', fontsize = 20)\n",
    "plt.ylabel('Price', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Price by Year Built', fontsize = 20)\n",
    "plt.scatter(df_train['Year Built'], df_train['SalePrice'])\n",
    "b, m = polyfit(df_train['Year Built'], df_train['SalePrice'], 1)\n",
    "plt.plot(df_train['Year Built'], b + m * df_train['Year Built'], '-', color = 'red')\n",
    "# Slight linear relationship can be see. More recent renovation = generally higher price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exploring above grade living area\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.xlabel('Above Grade Live Area (SF)', fontsize = 20)\n",
    "plt.ylabel('Price', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Price by Above Grade Live Area', fontsize = 20)\n",
    "plt.scatter(df_train['Gr Liv Area'], df_train['SalePrice'])\n",
    "b, m = polyfit(df_train['Gr Liv Area'], df_train['SalePrice'], 1)\n",
    "plt.plot(df_train['Gr Liv Area'], b + m * df_train['Gr Liv Area'], '-', color = 'red')\n",
    "\n",
    "# Slight linear relationship can be see. More recent renovation = generally higher price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for the neighborhood variable\n",
    "neighborhoods = pd.get_dummies(df_train['Neighborhood'])\n",
    "neighborhoods['SalePrice'] = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average price by neighborhood\n",
    "neighMeans = []\n",
    "for column in neighborhoods.columns:\n",
    "    mask = df_train['Neighborhood'] == column\n",
    "    temp = df_train[mask]['SalePrice'].mean()\n",
    "    neighMeans.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe for easier manipulation\n",
    "neighborMean = pd.DataFrame()\n",
    "neighborMean['Neighborhoods'] = neighborhoods.columns.values\n",
    "neighborMean['Average Price'] = neighMeans\n",
    "neighborMean.drop(index = 28, inplace = True)\n",
    "neighborMean = neighborMean.sort_values('Average Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking neighborhoods vs the average price\n",
    "neighborMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning lists to be used in histogram plot, and plotting as well\n",
    "neghborhoodList = list(neighborMean['Neighborhoods'].values)\n",
    "avgPriceList = list(neighborMean['Average Price'].values)\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.xticks(rotation='vertical', fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlabel('Neighborhoods', fontsize = 20)\n",
    "plt.ylabel('Price', fontsize = 20)\n",
    "plt.title('Average Price by Neighborhoods', fontsize = 20)\n",
    "plt.bar(neghborhoodList, avgPriceList,align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for all categorical values\n",
    "dummyCat = pd.get_dummies(df_train[catColumns]).columns.tolist()\n",
    "df_train = pd.get_dummies(df_train, columns = catColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for all categorical values\n",
    "dummyCatTest = pd.get_dummies(df_test[catColumns]).columns.tolist()\n",
    "df_test = pd.get_dummies(df_test, columns = catColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to ensure that data that can be found in train match the ones that can be found in test otherwise the data\n",
    "# will not be able to be split properly during modeling section\n",
    "# Setting columns to remove from train \n",
    "toRemoveTrain = list(set(df_train)-set(df_test))\n",
    "toRemoveTrain.remove('SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting columnes to remove from test\n",
    "toRemoveTest = list(set(df_test)-set(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy categories list to add to features later on\n",
    "dummyCat = list(set(dummyCat) - set(toRemoveTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting train data for baseline model. For the baseline, we will only be using total square footage. \n",
    "X = df_train['Total SF']\n",
    "y = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for baseline model, just using sale price and tota\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Instantiate\n",
    "base_mean = DummyRegressor(strategy = 'mean')\n",
    "\n",
    "# Fit\n",
    "base_mean = base_mean.fit(X_train, y_train)\n",
    "\n",
    "# Get Predictions\n",
    "y_hat_base_train = base_mean.predict(X_train)\n",
    "y_hat_base_test = base_mean.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "print(\"Train RMSE score:\", np.sqrt(mean_squared_error(y_train, y_hat_base_train)))\n",
    "print(\"Test RMSE score:\", np.sqrt(mean_squared_error(y_test, y_hat_base_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting train data\n",
    "features = ['Overall Qual', 'Total SF', 'Year Remod/Add', 'Year Built']\n",
    "features += dummyCat\n",
    "features += ordinal_cols\n",
    "X = df_train[features]\n",
    "y = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building linear regression model\n",
    "\n",
    "# Instantiate\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "our_model = lr.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_hat_lr_train = our_model.predict(X_train)\n",
    "y_hat_lr_test = our_model.predict(X_test)\n",
    "\n",
    "# Calculate the Root Mean Squared Error for each (first one is train, and second one is test)\n",
    "print(\"Train RMSE score:\", np.sqrt(mean_squared_error(y_train, y_hat_lr_train)))\n",
    "print(\"Test RMSE score:\", np.sqrt(mean_squared_error(y_test, y_hat_lr_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation Scoring\n",
    "cv_scores = cross_val_score(lr, X_train, y_train, cv = 5) # cv is how many cross validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation score means\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling X for the ridge and lasso regressions\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaledX_train = ss.fit_transform(X_train)\n",
    "scaledX_test = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "# Instantiate.\n",
    "# First test to use alpha = 10\n",
    "ridge_model = Ridge(alpha = 10)\n",
    "\n",
    "# Fit.\n",
    "ridge_model.fit(scaledX_train, y_train)\n",
    "\n",
    "# Generate predictions.\n",
    "# ridge_preds = ridge_model.predict(X_overfit_test)\n",
    "ridge_preds_train = ridge_model.predict(scaledX_train)\n",
    "\n",
    "# Evaluate model using R2.\n",
    "# print(r2_score(y_overfit_test, ridge_preds))\n",
    "print(r2_score(y_train, ridge_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of ridge alphas to check.\n",
    "r_alphas = np.logspace(0, 5, 100)\n",
    "# Generates 100 values equally between 0 and 5,\n",
    "# then converts them to alphas between 10^0 and 10^5.\n",
    "\n",
    "# Cross-validate over our list of ridge alphas.\n",
    "ridge_model = RidgeCV(alphas=r_alphas, scoring='r2', cv=5)\n",
    "\n",
    "# Fit model using best ridge alpha!\n",
    "ridge_model = ridge_model.fit(scaledX_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the optimal value of alpha\n",
    "ridge_optimal_alpha = ridge_model.alpha_\n",
    "ridge_optimal_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n",
    "our_model = Ridge(alpha=ridge_optimal_alpha)\n",
    "\n",
    "# Fit model.\n",
    "our_model.fit(scaledX_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "ridge_opt_preds = our_model.predict(scaledX_test)\n",
    "ridge_opt_preds_train = our_model.predict(scaledX_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(\"Train RMSE score:\", np.sqrt(mean_squared_error(y_train, ridge_opt_preds_train)))\n",
    "print(\"Test RMSE score:\", np.sqrt(mean_squared_error(y_test, ridge_opt_preds)))\n",
    "print(r2_score(y_test, ridge_opt_preds))\n",
    "print(r2_score(y_train, ridge_opt_preds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of Lasso alphas to check.\n",
    "l_alphas = np.linspace(0.15, 1, 85)\n",
    "# Generates 85 values equally between 0.15 and 1.\n",
    "\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "our_model = LassoCV(alphas=l_alphas, cv=5)\n",
    "\n",
    "# Fit model using best ridge alpha!\n",
    "our_model = our_model.fit(scaledX_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the optimal value of alpha\n",
    "lasso_optimal_alpha = our_model.alpha_\n",
    "lasso_optimal_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "lasso_model_preds = our_model.predict(scaledX_test)\n",
    "lasso_model_preds_train = our_model.predict(scaledX_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(\"Train RMSE score:\", np.sqrt(mean_squared_error(y_train, lasso_model_preds_train)))\n",
    "print(\"Test RMSE score:\", np.sqrt(mean_squared_error(y_test, lasso_model_preds)))\n",
    "print(r2_score(y_test, lasso_model_preds))\n",
    "print(r2_score(y_train, lasso_model_preds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Predictions Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating kaggle test prediction stuff\n",
    "\n",
    "X_kaggle = df_test[features] # Setting testing set of features\n",
    "\n",
    "# Scaled x kaggle \n",
    "scaledX_kaggle = ss.fit_transform(X_kaggle)\n",
    "\n",
    "X_kaggle['SalePrice'] = our_model.predict(scaledX_kaggle) # creating new column called saleprice which is what we want, \n",
    "# just have to apply already built model to this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle.drop(features, axis=1, inplace=True) # keeping only ID and the predicted sales price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if X_kaggle has been created properly\n",
    "X_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_kaggle.index, X_kaggle['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting X_kaggle to csv file for submission at kaggle\n",
    "#X_kaggle.to_csv(\"./datasets/Final_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all total square footage, overall quality, year built, year remodeled, all the ordinal and categorical \n",
    "# variables will give the predicted prices. However, during actual house hunting, the best thing to do would be to \n",
    "# look only for the total square footage, overall quality, year built, and year remodeled as main variables since \n",
    "# majority of the other variables supports those four main variables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
